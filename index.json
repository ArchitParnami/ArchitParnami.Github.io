[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m currently a PhD student at The University of North Carolina at Charlotte working under the supervision of Dr. Minwoo Lee. I am interested in machine learning and computer vision, particularly my research is focused on the subfields of machine learning i.e., few-shot learning, meta-learning and continual learning in neural networks.\n","date":1596489192,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1596489192,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://ArchitParnami.github.io/author/archit-parnami/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/archit-parnami/","section":"authors","summary":"I\u0026rsquo;m currently a PhD student at The University of North Carolina at Charlotte working under the supervision of Dr. Minwoo Lee. I am interested in machine learning and computer vision, particularly my research is focused on the subfields of machine learning i.","tags":null,"title":"Archit Parnami","type":"authors"},{"authors":["Archit Parnami"],"categories":["Paper Implementation"],"content":"","date":1596489192,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596489192,"objectID":"4aaf76d3d96cdd3a726474acd457372b","permalink":"https://ArchitParnami.github.io/project/dp-pix/","publishdate":"2020-08-03T17:13:12-04:00","relpermalink":"/project/dp-pix/","section":"project","summary":"","tags":["Differential Privacy","Image Pixelation"],"title":"Image Pixelization with Differential Privacy","type":"project"},{"authors":["Archit Parnami"],"categories":["Few-Shot Learning"],"content":"","date":1596488329,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596488329,"objectID":"5c6cb4c8b4e9483e7ef26ba3f11333fb","permalink":"https://ArchitParnami.github.io/publication/fs-kws/","publishdate":"2020-08-03T16:58:49-04:00","relpermalink":"/publication/fs-kws/","section":"publication","summary":"Recognizing a particular command or a keyword, keyword spotting has been widely used in many voice interfaces such as Amazon's Alexa and Google Home. In order to recognize a set of keywords, most of the recent deep learning based approaches use a neural network trained with a large number of samples to identify certain pre-defined keywords. This restricts the system from recognizing new, user-defined keywords. Therefore, we first formulate this problem as a few-shot keyword spotting and approach it using metric learning. To enable this research, we also synthesize and publish a Few-shot Google Speech Commands dataset. We then propose a solution to the few-shot keyword spotting problem using temporal and dilated convolutions on prototypical networks. Our comparative experimental results demonstrate keyword spotting of new keywords using just a small number of samples. ","tags":["Keyword Spotting","Speech Recognition","Audio"],"title":"Few-Shot Keyword Spotting With Prototypical Networks","type":"publication"},{"authors":["Archit Parnami"],"categories":["Few-Shot-Learning"],"content":"   Paper https://arxiv.org/abs/2006.15486     Code https://github.com/imtiazziko/LaplacianShot    Main Idea   Transfer Learning: Image embeddings are obtained by pre-training a network on the set of base classes using cross-entropy loss.\n  Transductive Inference: Jointly classify all the query examples together.\n  Methodology Given a labeled support set $\\mathbb{X_s} = \\bigcup_{c=1}^C$ with C _test_ classes, where each novel class $c$ has $|\\mathbb{X_s^c}|$ labeled examples. Ex., $|\\mathbb{X_s^c}| = 1$ for one-shot learning. The object is then to classify unlabeled unseen query sample set $|\\mathbb{X_q}|$.\nLet $f_{\\theta}$ be the embedding function with parameters $\\theta$. Then:\n$$x_q = f_{\\theta}(z_q) \\in \\mathbb{R}^M,$$ where $x_q$ is the encoding of the data point $z_q$.\nFor each query point $x_q$, let there be a assignment vector $\\mathbf{y_q} = [y_{q,1}, \u0026hellip;, y_{q,C}]^t \\in {0,1}^C$\nso that binary $y_{q,c}$ is equal to 1 if $x_q$ belongs to class $c$. Then $\\mathbf{Y}$ denotes the $N \\times C$ matrix whose rows are formed by $\\mathbf{y}_q$, where $N$ is the number of query points in $\\mathbb{X_q}$. Then the objective to minimize at inference is given by (to find a $\\mathbf{Y}$ such that):\n$$\\mathcal{E}(\\mathbf{Y}) = \\mathcal{N}(\\mathbf{Y}) + \\frac{\\lambda}{2} \\mathcal{L}(\\mathbf{Y})$$\nwhere\n$$\\mathcal{N}(\\mathbf{Y}) = \\sum_{q=1}^N \\sum_{c=1}^C y_{q,c} d(x_q - \\mathbf{m}_c)$$\nand\n$$\\mathcal{L}(\\mathbf{Y}) = \\frac{1}{2} \\sum_{q,p} w(x_q, x_p) || \\mathbf{y}_q - \\mathbf{y}_p||^2$$\n $\\mathcal{N}(\\mathbf{Y})$ is minimized globally when each query point is assigned to the class of the nearest prototype $\\mathbf{m}_c$ from the support set using a distance metric $d(x_q, m_c)$. The Laplacian term (regularizer), $\\mathcal{L}(\\mathbf{Y})$, encourages nearby points $(x_p, x_q)$ in the label space to the same latent label assignment. $w$ is any similarity metric. $\\lambda$ is regularization parameter whose value is found by measuring performance on validation set.  Using an iterative bound optimization procedure (Ex., Expectation-Maximization (EM)), $\\mathcal{E}(\\mathbf{Y})$ is minimized and $\\mathbf{Y}$ is found. Their optimization procedure converges within 15 iterations. Please refer to the paper for details on optimization method.\nResults    Network dataset 1-shot 5-shot     ResNet-18 miniImageNet $72.11 \\pm 0.19$ $82.31 \\pm 0.14$   ResNet-18 tieredImageNet $78.98 \\pm 0.21$ $86.39 \\pm 0.16$   ResNet-18 CUB $80.96$ $88.68$   ResNet-18 miniImageNet $\\rightarrow$ CUB1 $55.46$ $66.33$    Cite / BibTex @article{Ziko2020LaplacianRF, title={Laplacian Regularized Few-Shot Learning}, author={Imtiaz Masud Ziko and Jose Dolz and {\\'E}ric Granger and Ismail Ben Ayed}, journal={ArXiv}, year={2020}, volume={abs/2006.15486} }    Cross Domain FSL: Training on miniImageNet and testing on CUB. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1596487158,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596487158,"objectID":"13e598ed44a571f2753b3c78eb855bfb","permalink":"https://ArchitParnami.github.io/post/laplacian-fsl/","publishdate":"2020-08-03T16:39:18-04:00","relpermalink":"/post/laplacian-fsl/","section":"post","summary":"Paper https://arxiv.org/abs/2006.15486     Code https://github.com/imtiazziko/LaplacianShot    Main Idea   Transfer Learning: Image embeddings are obtained by pre-training a network on the set of base classes using cross-entropy loss.","tags":["ICML","2020","Transductive","Transfer-Learning"],"title":"Laplacian Regularized Few-Shot Learning","type":"post"}]